{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682d4d11",
   "metadata": {},
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuation.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Remove stopwords (using NLTK's stopwords list).\n",
    "4. Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55917516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      "['technology has always fascinated me especially how it keeps evolving to make life easier \\nfrom smartphones and smartwatches to artificial intelligence and robotics the possibilities seem endless \\ni enjoy learning about the latest gadgets and how they’re built to solve realworld problems \\nthe pace of innovation in this field is incredible and it’s exciting to imagine what the future holds \\nwhether it’s a new app or a groundbreaking invention technology never fails to amaze me']\n",
      "\n",
      "Filtered Words (no stopwords):\n",
      "['technology', 'always', 'fascinated', 'especially', 'keeps', 'evolving', 'make', 'life', 'easier', 'smartphones', 'smartwatches', 'artificial', 'intelligence', 'robotics', 'possibilities', 'seem', 'endless', 'enjoy', 'learning', 'latest', 'gadgets', '’', 'built', 'solve', 'realworld', 'problems', 'pace', 'innovation', 'field', 'incredible', '’', 'exciting', 'imagine', 'future', 'holds', 'whether', '’', 'new', 'app', 'groundbreaking', 'invention', 'technology', 'never', 'fails', 'amaze']\n",
      "\n",
      "Word Frequency Distribution:\n",
      "[('’', 3), ('technology', 2), ('always', 1), ('fascinated', 1), ('especially', 1), ('keeps', 1), ('evolving', 1), ('make', 1), ('life', 1), ('easier', 1), ('smartphones', 1), ('smartwatches', 1), ('artificial', 1), ('intelligence', 1), ('robotics', 1), ('possibilities', 1), ('seem', 1), ('endless', 1), ('enjoy', 1), ('learning', 1), ('latest', 1), ('gadgets', 1), ('built', 1), ('solve', 1), ('realworld', 1), ('problems', 1), ('pace', 1), ('innovation', 1), ('field', 1), ('incredible', 1), ('exciting', 1), ('imagine', 1), ('future', 1), ('holds', 1), ('whether', 1), ('new', 1), ('app', 1), ('groundbreaking', 1), ('invention', 1), ('never', 1), ('fails', 1), ('amaze', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"\"\"Technology has always fascinated me, especially how it keeps evolving to make life easier. \n",
    "From smartphones and smartwatches to artificial intelligence and robotics, the possibilities seem endless. \n",
    "I enjoy learning about the latest gadgets and how they’re built to solve real-world problems. \n",
    "The pace of innovation in this field is incredible, and it’s exciting to imagine what the future holds. \n",
    "Whether it’s a new app or a groundbreaking invention, technology never fails to amaze me.\"\"\"\n",
    "\n",
    "text_lower_nopunct = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "words = word_tokenize(text_lower_nopunct)\n",
    "sentences = sent_tokenize(text_lower_nopunct)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "fdist = FreqDist(filtered_words)\n",
    "\n",
    "print(\"Tokenized Sentences:\")\n",
    "print(sentences)\n",
    "print(\"\\nFiltered Words (no stopwords):\")\n",
    "print(filtered_words)\n",
    "print(\"\\nWord Frequency Distribution:\")\n",
    "print(fdist.most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2bcf18",
   "metadata": {},
   "source": [
    "Q2: Stemming and Lemmatization\n",
    "1. Take the tokenized words from Question 1 (after stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmatization using NLTK's WordNetLemmatizer.\n",
    "4. Compare and display results of both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73d7d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words (after stopword removal):\n",
      "['technology', 'always', 'fascinated', 'especially', 'keeps', 'evolving', 'make', 'life', 'easier', 'smartphones', 'smartwatches', 'artificial', 'intelligence', 'robotics', 'possibilities', 'seem', 'endless', 'enjoy', 'learning', 'latest', 'gadgets', '’', 'built', 'solve', 'realworld', 'problems', 'pace', 'innovation', 'field', 'incredible', '’', 'exciting', 'imagine', 'future', 'holds', 'whether', '’', 'new', 'app', 'groundbreaking', 'invention', 'technology', 'never', 'fails', 'amaze']\n",
      "\n",
      "Porter Stemmer:\n",
      "['technolog', 'alway', 'fascin', 'especi', 'keep', 'evolv', 'make', 'life', 'easier', 'smartphon', 'smartwatch', 'artifici', 'intellig', 'robot', 'possibl', 'seem', 'endless', 'enjoy', 'learn', 'latest', 'gadget', '’', 'built', 'solv', 'realworld', 'problem', 'pace', 'innov', 'field', 'incred', '’', 'excit', 'imagin', 'futur', 'hold', 'whether', '’', 'new', 'app', 'groundbreak', 'invent', 'technolog', 'never', 'fail', 'amaz']\n",
      "\n",
      "Lancaster Stemmer:\n",
      "['technolog', 'alway', 'fascin', 'espec', 'keep', 'evolv', 'mak', 'lif', 'easy', 'smartphon', 'smartwatch', 'art', 'intellig', 'robot', 'poss', 'seem', 'endless', 'enjoy', 'learn', 'latest', 'gadget', '’', 'built', 'solv', 'realworld', 'problem', 'pac', 'innov', 'field', 'incred', '’', 'excit', 'imagin', 'fut', 'hold', 'wheth', '’', 'new', 'ap', 'groundbreak', 'inv', 'technolog', 'nev', 'fail', 'amaz']\n",
      "\n",
      "WordNet Lemmatizer:\n",
      "['technology', 'always', 'fascinated', 'especially', 'keep', 'evolving', 'make', 'life', 'easier', 'smartphones', 'smartwatches', 'artificial', 'intelligence', 'robotics', 'possibility', 'seem', 'endless', 'enjoy', 'learning', 'latest', 'gadget', '’', 'built', 'solve', 'realworld', 'problem', 'pace', 'innovation', 'field', 'incredible', '’', 'exciting', 'imagine', 'future', 'hold', 'whether', '’', 'new', 'app', 'groundbreaking', 'invention', 'technology', 'never', 'fails', 'amaze']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') \n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter_stems = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_stems = [lancaster.stem(word) for word in filtered_words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Original Words (after stopword removal):\")\n",
    "print(filtered_words)\n",
    "print(\"\\nPorter Stemmer:\")\n",
    "print(porter_stems)\n",
    "print(\"\\nLancaster Stemmer:\")\n",
    "print(lancaster_stems)\n",
    "print(\"\\nWordNet Lemmatizer:\")\n",
    "print(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e8cb9",
   "metadata": {},
   "source": [
    "Q3. Regular Expressions and Text Spliƫng\n",
    "1. Take their original text from Question 1.\n",
    "2. Use regular expressions to:\n",
    "2. Extract all words with more than 5 letters.\n",
    "2. Extract all numbers (if any exist in their text).\n",
    "2. Extract all capitalized words.\n",
    "3. Use text spliƫng techniques to:\n",
    "3. Split the text into words containing only alphabets (removing digits and special characters).\n",
    "3. Extract words starting with a vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13b774ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with more than 5 letters:\n",
      "['Technology', 'always', 'fascinated', 'especially', 'evolving', 'easier', 'smartphones', 'smartwatches', 'artificial', 'intelligence', 'robotics', 'possibilities', 'endless', 'learning', 'latest', 'gadgets', 'problems', 'innovation', 'incredible', 'exciting', 'imagine', 'future', 'Whether', 'groundbreaking', 'invention', 'technology']\n",
      "\n",
      "Numbers in the text:\n",
      "[]\n",
      "\n",
      "Capitalized Words:\n",
      "['Technology', 'From', 'I', 'The', 'Whether']\n",
      "\n",
      "Words with only alphabets (no digits/special chars):\n",
      "['Technology', 'has', 'always', 'fascinated', 'me', 'especially', 'how', 'it', 'keeps', 'evolving', 'to', 'make', 'life', 'easier', 'From', 'smartphones', 'and', 'smartwatches', 'to', 'artificial', 'intelligence', 'and', 'robotics', 'the', 'possibilities', 'seem', 'endless', 'I', 'enjoy', 'learning', 'about', 'the', 'latest', 'gadgets', 'and', 'how', 'they', 're', 'built', 'to', 'solve', 'real', 'world', 'problems', 'The', 'pace', 'of', 'innovation', 'in', 'this', 'field', 'is', 'incredible', 'and', 'it', 's', 'exciting', 'to', 'imagine', 'what', 'the', 'future', 'holds', 'Whether', 'it', 's', 'a', 'new', 'app', 'or', 'a', 'groundbreaking', 'invention', 'technology', 'never', 'fails', 'to', 'amaze', 'me']\n",
      "\n",
      "Words starting with a vowel:\n",
      "['always', 'especially', 'it', 'evolving', 'easier', 'and', 'artificial', 'intelligence', 'and', 'endless', 'I', 'enjoy', 'about', 'and', 'of', 'innovation', 'in', 'is', 'incredible', 'and', 'it', 'exciting', 'imagine', 'it', 'a', 'app', 'or', 'a', 'invention', 'amaze']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"Technology has always fascinated me, especially how it keeps evolving to make life easier. \n",
    "From smartphones and smartwatches to artificial intelligence and robotics, the possibilities seem endless. \n",
    "I enjoy learning about the latest gadgets and how they’re built to solve real-world problems. \n",
    "The pace of innovation in this field is incredible, and it’s exciting to imagine what the future holds. \n",
    "Whether it’s a new app or a groundbreaking invention, technology never fails to amaze me.\"\"\"\n",
    "words_gt5 = re.findall(r'\\b[a-zA-Z]{6,}\\b', text)\n",
    "numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "vowel_words = [word for word in alphabetic_words if re.match(r'^[aeiouAEIOU]', word)]\n",
    "print(\"Words with more than 5 letters:\")\n",
    "print(words_gt5)\n",
    "print(\"\\nNumbers in the text:\")\n",
    "print(numbers)\n",
    "print(\"\\nCapitalized Words:\")\n",
    "print(capitalized_words)\n",
    "print(\"\\nWords with only alphabets (no digits/special chars):\")\n",
    "print(alphabetic_words)\n",
    "print(\"\\nWords starting with a vowel:\")\n",
    "print(vowel_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8a9dc",
   "metadata": {},
   "source": [
    "Q4. Custom Tokenization & Regex-based Text Cleaning\n",
    "1. Take original text from QuesƟon 1.\n",
    "2. Write a custom tokenization funcƟon that:\n",
    "a. Removes punctuation and special symbols, but keeps contractions (e.g.,\n",
    "\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains\n",
    "a single token).\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\"\n",
    "should remain as is).\n",
    "\n",
    "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
    "a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "b. Replace URLs with '<URL>' placeholder.\n",
    "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with\n",
    "'<PHONE>' placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d50a6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['Contact', 'us', 'at', 'EMAIL', 'or', 'visit', 'our', 'website', 'URL', 'for', 'more', 'info', 'You', 'can', 'also', 'call', '91', '9876543210', 'or', 'PHONE', 'This', \"isn't\", 'state-of-the-art', 'but', \"it's\", 'affordable', 'The', 'value', 'is', '3.14']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Contact us at support@example.com or visit our website https://www.example.com for more info. You can also call +91 9876543210 or 123-456-7890. This isn't state-of-the-art, but it's affordable. The value is 3.14.\"\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '<EMAIL>', text)\n",
    "    text = re.sub(r'(https?://[^\\s]+)', '<URL>', text)\n",
    "    text = re.sub(r'\\b(?:\\+91\\s?\\d{10}|(?:\\d{3}-){2}\\d{4})\\b', '<PHONE>', text)\n",
    "    return text\n",
    "def custom_tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    token_pattern = r\"\"\"\\b\\w+(?:-\\w+)+\\b|\\b\\w+'\\w+\\b|\\b\\d+\\.\\d+\\b|\\b\\w+\\b\"\"\"\n",
    "    tokens = re.findall(token_pattern, text, re.VERBOSE)\n",
    "    return tokens\n",
    "tokens = custom_tokenize(text)\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
