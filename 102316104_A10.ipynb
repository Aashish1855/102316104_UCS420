{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bf27bc",
   "metadata": {},
   "source": [
    "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
    "technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuation using re.\n",
    "2. Tokenize the text into words and sentences.\n",
    "3. Split using split() and word_tokenize() and compare how Python split and NLTK’s\n",
    "word_tokenize() differ.\n",
    "4. Remove stopwords (using NLTK's stopwords list).\n",
    "5. Display word frequency distribution (excluding stopwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33fb5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " Technology is evolving faster than ever. Artificial Intelligence is reshaping how we interact with machines. I love exploring new gadgets and learning about innovations. From smartphones to smart homes, tech is deeply embedded in our daily lives. It excites me to see how automation and data science are transforming industries.\n",
      "\n",
      "Cleaned Text:\n",
      " technology is evolving faster than ever artificial intelligence is reshaping how we interact with machines i love exploring new gadgets and learning about innovations from smartphones to smart homes tech is deeply embedded in our daily lives it excites me to see how automation and data science are transforming industries\n",
      "\n",
      "Sentences:\n",
      " ['Technology is evolving faster than ever.', 'Artificial Intelligence is reshaping how we interact with machines.', 'I love exploring new gadgets and learning about innovations.', 'From smartphones to smart homes, tech is deeply embedded in our daily lives.', 'It excites me to see how automation and data science are transforming industries.']\n",
      "\n",
      "Python split():\n",
      " ['technology', 'is', 'evolving', 'faster', 'than', 'ever', 'artificial', 'intelligence', 'is', 'reshaping', 'how', 'we', 'interact', 'with', 'machines', 'i', 'love', 'exploring', 'new', 'gadgets', 'and', 'learning', 'about', 'innovations', 'from', 'smartphones', 'to', 'smart', 'homes', 'tech', 'is', 'deeply', 'embedded', 'in', 'our', 'daily', 'lives', 'it', 'excites', 'me', 'to', 'see', 'how', 'automation', 'and', 'data', 'science', 'are', 'transforming', 'industries']\n",
      "\n",
      "NLTK word_tokenize():\n",
      " ['technology', 'is', 'evolving', 'faster', 'than', 'ever', 'artificial', 'intelligence', 'is', 'reshaping', 'how', 'we', 'interact', 'with', 'machines', 'i', 'love', 'exploring', 'new', 'gadgets', 'and', 'learning', 'about', 'innovations', 'from', 'smartphones', 'to', 'smart', 'homes', 'tech', 'is', 'deeply', 'embedded', 'in', 'our', 'daily', 'lives', 'it', 'excites', 'me', 'to', 'see', 'how', 'automation', 'and', 'data', 'science', 'are', 'transforming', 'industries']\n",
      "\n",
      "Stopwords Removed:\n",
      " ['technology', 'evolving', 'faster', 'ever', 'artificial', 'intelligence', 'reshaping', 'interact', 'machines', 'love', 'exploring', 'new', 'gadgets', 'learning', 'innovations', 'smartphones', 'smart', 'homes', 'tech', 'deeply', 'embedded', 'daily', 'lives', 'excites', 'see', 'automation', 'data', 'science', 'transforming', 'industries']\n",
      "\n",
      "Word Frequency Distribution:\n",
      " Counter({'technology': 1, 'evolving': 1, 'faster': 1, 'ever': 1, 'artificial': 1, 'intelligence': 1, 'reshaping': 1, 'interact': 1, 'machines': 1, 'love': 1, 'exploring': 1, 'new': 1, 'gadgets': 1, 'learning': 1, 'innovations': 1, 'smartphones': 1, 'smart': 1, 'homes': 1, 'tech': 1, 'deeply': 1, 'embedded': 1, 'daily': 1, 'lives': 1, 'excites': 1, 'see': 1, 'automation': 1, 'data': 1, 'science': 1, 'transforming': 1, 'industries': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "text = \"Technology is evolving faster than ever. Artificial Intelligence is reshaping how we interact with machines. I love exploring new gadgets and learning about innovations. From smartphones to smart homes, tech is deeply embedded in our daily lives. It excites me to see how automation and data science are transforming industries.\"\n",
    "cleaned_text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "sentences = sent_tokenize(text)\n",
    "word_tokens_nltk = word_tokenize(cleaned_text)\n",
    "word_tokens_split = cleaned_text.split()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in word_tokens_nltk if word not in stop_words]\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"Original Text:\\n\", text)\n",
    "print(\"\\nCleaned Text:\\n\", cleaned_text)\n",
    "print(\"\\nSentences:\\n\", sentences)\n",
    "print(\"\\nPython split():\\n\", word_tokens_split)\n",
    "print(\"\\nNLTK word_tokenize():\\n\", word_tokens_nltk)\n",
    "print(\"\\nStopwords Removed:\\n\", filtered_words)\n",
    "print(\"\\nWord Frequency Distribution:\\n\", word_freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d098e7a",
   "metadata": {},
   "source": [
    "Q2. Using the same paragraph from Q1:\n",
    "1. Extract all words with only alphabets using re.findall()\n",
    "2. Remove stop words using NLTK’s stopword list\n",
    "3. Perform stemming with PorterStemmer\n",
    "4. Perform lemmatization with WordNetLemmatizer\n",
    "5. Compare the stemmed and lemmatized outputs and explain when you’d prefer one over\n",
    "the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9c3d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with only alphabets: ['technology', 'is', 'evolving', 'faster', 'than', 'ever', 'artificial', 'intelligence', 'is', 'reshaping', 'how', 'we', 'interact', 'with', 'machines', 'i', 'love', 'exploring', 'new', 'gadgets', 'and', 'learning', 'about', 'innovations', 'from', 'smartphones', 'to', 'smart', 'homes', 'tech', 'is', 'deeply', 'embedded', 'in', 'our', 'daily', 'lives', 'it', 'excites', 'me', 'to', 'see', 'how', 'automation', 'and', 'data', 'science', 'are', 'transforming', 'industries']\n",
      "Words after removing stop words: ['technology', 'evolving', 'faster', 'ever', 'artificial', 'intelligence', 'reshaping', 'interact', 'machines', 'love', 'exploring', 'new', 'gadgets', 'learning', 'innovations', 'smartphones', 'smart', 'homes', 'tech', 'deeply', 'embedded', 'daily', 'lives', 'excites', 'see', 'automation', 'data', 'science', 'transforming', 'industries']\n",
      "Stemmed words: ['technolog', 'evolv', 'faster', 'ever', 'artifici', 'intellig', 'reshap', 'interact', 'machin', 'love', 'explor', 'new', 'gadget', 'learn', 'innov', 'smartphon', 'smart', 'home', 'tech', 'deepli', 'embed', 'daili', 'live', 'excit', 'see', 'autom', 'data', 'scienc', 'transform', 'industri']\n",
      "Lemmatized words: ['technology', 'evolving', 'faster', 'ever', 'artificial', 'intelligence', 'reshaping', 'interact', 'machine', 'love', 'exploring', 'new', 'gadget', 'learning', 'innovation', 'smartphones', 'smart', 'home', 'tech', 'deeply', 'embedded', 'daily', 'life', 'excites', 'see', 'automation', 'data', 'science', 'transforming', 'industry']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "text = \"Technology is evolving faster than ever. Artificial Intelligence is reshaping how we interact with machines. I love exploring new gadgets and learning about innovations. From smartphones to smart homes, tech is deeply embedded in our daily lives. It excites me to see how automation and data science are transforming industries.\"\n",
    "\n",
    "words = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "print(\"Words with only alphabets:\", words)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "print(\"Words after removing stop words:\", filtered_words)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(word) for word in filtered_words]\n",
    "print(\"Stemmed words:\", stemmed_words)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "print(\"Lemmatized words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf49f187",
   "metadata": {},
   "source": [
    "Q3. Choose 3 short texts of your own (e.g., different news headlines, product reviews).\n",
    "1. Use CountVectorizer to generate the Bag of Words representaƟon.\n",
    "2. Use TfidfVectorizer to compute TF-IDF scores.\n",
    "3. Print and interpret the top 3 keywords from each text using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47491b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation:\n",
      "    advanced  agriculture  ai  battery  boasts  camera  change  climate  \\\n",
      "0         1            0   1        0       0       0       0        0   \n",
      "1         0            0   0        1       1       1       0        0   \n",
      "2         0            1   0        0       0       0       1        1   \n",
      "\n",
      "   cutting  diagnostics  edge  global  healthcare  impacts  life  long  \\\n",
      "0        0            1     0       0           1        0     0     0   \n",
      "1        1            0     1       0           0        0     1     1   \n",
      "2        0            0     0       1           0        1     0     0   \n",
      "\n",
      "   negatively  revolutionizes  smartphone  \n",
      "0           0               1           0  \n",
      "1           0               0           1  \n",
      "2           1               0           0  \n",
      "\n",
      "TF-IDF Representation:\n",
      "    advanced  agriculture        ai   battery    boasts    camera    change  \\\n",
      "0  0.447214     0.000000  0.447214  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000     0.000000  0.000000  0.353553  0.353553  0.353553  0.000000   \n",
      "2  0.000000     0.408248  0.000000  0.000000  0.000000  0.000000  0.408248   \n",
      "\n",
      "    climate   cutting  diagnostics      edge    global  healthcare   impacts  \\\n",
      "0  0.000000  0.000000     0.447214  0.000000  0.000000    0.447214  0.000000   \n",
      "1  0.000000  0.353553     0.000000  0.353553  0.000000    0.000000  0.000000   \n",
      "2  0.408248  0.000000     0.000000  0.000000  0.408248    0.000000  0.408248   \n",
      "\n",
      "       life      long  negatively  revolutionizes  smartphone  \n",
      "0  0.000000  0.000000    0.000000        0.447214    0.000000  \n",
      "1  0.353553  0.353553    0.000000        0.000000    0.353553  \n",
      "2  0.000000  0.000000    0.408248        0.000000    0.000000  \n",
      "\n",
      "Text 1: AI revolutionizes healthcare with advanced diagnostics\n",
      "Top 3 keywords (TF-IDF): [('revolutionizes', np.float64(0.4472135954999579)), ('diagnostics', np.float64(0.4472135954999579)), ('healthcare', np.float64(0.4472135954999579))]\n",
      "\n",
      "Text 2: Smartphone boasts cutting-edge camera and long battery life\n",
      "Top 3 keywords (TF-IDF): [('smartphone', np.float64(0.3535533905932738)), ('life', np.float64(0.3535533905932738)), ('long', np.float64(0.3535533905932738))]\n",
      "\n",
      "Text 3: Climate change impacts global agriculture negatively\n",
      "Top 3 keywords (TF-IDF): [('negatively', np.float64(0.4082482904638631)), ('global', np.float64(0.4082482904638631)), ('impacts', np.float64(0.4082482904638631))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "texts = [\n",
    "    \"AI revolutionizes healthcare with advanced diagnostics\",\n",
    "    \"Smartphone boasts cutting-edge camera and long battery life\",\n",
    "    \"Climate change impacts global agriculture negatively\"\n",
    "]\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "bow_matrix = count_vectorizer.fit_transform(texts)\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "print(\"Bag of Words Representation:\\n\", bow_df)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Representation:\\n\", tfidf_df)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "for i, text in enumerate(texts):\n",
    "    tfidf_scores = tfidf_matrix[i].toarray().flatten()\n",
    "    top_indices = tfidf_scores.argsort()[-3:][::-1]\n",
    "    top_keywords = [(feature_names[idx], tfidf_scores[idx]) for idx in top_indices]\n",
    "    print(f\"\\nText {i+1}: {text}\")\n",
    "    print(\"Top 3 keywords (TF-IDF):\", top_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ca394",
   "metadata": {},
   "source": [
    "Q4. Write 2 short texts (4–6 lines each) describing two different technologies (e.g., AI vs\n",
    "Blockchain).\n",
    "1. Preprocess and tokenize both texts.\n",
    "2. Calculate:\n",
    "a. Jaccard Similarity using sets\n",
    "b. Cosine Similarity using TfidfVectorizer + cosine_similarity()\n",
    "c. Analyze which similarity metric gives beƩer insights in your case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4653ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for Text 1 (AI): ['artificial', 'intelligence', 'enhances', 'decisionmaking', 'machine', 'learning', 'processes', 'vast', 'datasets', 'predict', 'outcomes', 'accurately', 'ai', 'powers', 'applications', 'like', 'virtual', 'assistants', 'autonomous', 'vehicles', 'adaptability', 'drives', 'innovation', 'across', 'industries']\n",
      "Tokens for Text 2 (Blockchain): ['blockchain', 'ensures', 'secure', 'transparent', 'transactions', 'via', 'decentralized', 'ledgers', 'eliminates', 'intermediaries', 'reducing', 'costs', 'fraud', 'cryptocurrencies', 'like', 'bitcoin', 'rely', 'blockchain', 'technology', 'immutability', 'fosters', 'trust', 'digital', 'systems']\n",
      "\n",
      "Jaccard Similarity: 0.02127659574468085\n",
      "Cosine Similarity: 0.020657754435428406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Two short texts\n",
    "text1 = \"\"\"Artificial Intelligence enhances decision-making through machine learning. It processes vast datasets to predict outcomes accurately. AI powers applications like virtual assistants and autonomous vehicles. Its adaptability drives innovation across industries.\"\"\"\n",
    "text2 = \"\"\"Blockchain ensures secure, transparent transactions via decentralized ledgers. It eliminates intermediaries, reducing costs and fraud. Cryptocurrencies like Bitcoin rely on blockchain technology. Its immutability fosters trust in digital systems.\"\"\"\n",
    "\n",
    "# 1. Preprocess and tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "tokens1 = preprocess(text1)\n",
    "tokens2 = preprocess(text2)\n",
    "print(\"Tokens for Text 1 (AI):\", tokens1)\n",
    "print(\"Tokens for Text 2 (Blockchain):\", tokens2)\n",
    "\n",
    "# 2a. Jaccard Similarity\n",
    "set1, set2 = set(tokens1), set(tokens2)\n",
    "jaccard_sim = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "print(\"\\nJaccard Similarity:\", jaccard_sim)\n",
    "\n",
    "# 2b. Cosine Similarity with TfidfVectorizer\n",
    "texts = [text1, text2]\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "print(\"Cosine Similarity:\", cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea52a3",
   "metadata": {},
   "source": [
    "Q5. Write a short review for a product or service.\n",
    "1. Use TextBlob or VADER to find polarity & subjecƟvity for each review.\n",
    "2. Classify reviews into PosiƟve / NegaƟve / Neutral.\n",
    "3. Create a word cloud using the wordcloud library for all posiƟve reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa306331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aashishsharma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextBlob - Polarity: 0.357 Subjectivity: 0.508\n",
      "VADER - Compound Score: 0.7776\n",
      "Review Classification: Positive\n",
      "Word cloud generated and saved as 'wordcloud_positive_review.png'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Review text\n",
    "review = \"The SmartHome Hub is incredibly user-friendly and seamlessly connects all my devices. Setup was a breeze, and the app is intuitive. It saves time and makes daily tasks effortless. Highly recommend this innovative product!\"\n",
    "\n",
    "# 1. Sentiment Analysis\n",
    "# TextBlob\n",
    "blob = TextBlob(review)\n",
    "textblob_polarity = blob.sentiment.polarity\n",
    "textblob_subjectivity = blob.sentiment.subjectivity\n",
    "print(\"TextBlob - Polarity:\", textblob_polarity, \"Subjectivity:\", textblob_subjectivity)\n",
    "\n",
    "# VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vader_scores = analyzer.polarity_scores(review)\n",
    "vader_compound = vader_scores['compound']\n",
    "print(\"VADER - Compound Score:\", vader_compound)\n",
    "\n",
    "# 2. Classify Review\n",
    "def classify_review(textblob_pol, vader_comp):\n",
    "    # Average polarity for robust classification\n",
    "    avg_polarity = (textblob_pol + vader_comp) / 2\n",
    "    if avg_polarity > 0.1:\n",
    "        return \"Positive\"\n",
    "    elif avg_polarity < -0.1:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "classification = classify_review(textblob_polarity, vader_compound)\n",
    "print(\"Review Classification:\", classification)\n",
    "\n",
    "# 3. Word Cloud for Positive Reviews\n",
    "if classification == \"Positive\":\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', min_font_size=10).generate(review)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Word Cloud for Positive Review\")\n",
    "    plt.savefig('wordcloud_positive_review.png')\n",
    "    plt.close()\n",
    "    print(\"Word cloud generated and saved as 'wordcloud_positive_review.png'\")\n",
    "else:\n",
    "    print(\"No word cloud generated as the review is not positive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d6f35",
   "metadata": {},
   "source": [
    "Q6. Choose your own paragraph (~100 words) as training data.\n",
    "1. Tokenize text using Tokenizer() from keras.preprocessing.text\n",
    "2. Create input sequences and build a simple LSTM or Dense model\n",
    "3. Train the model and generate 2–3 new lines of text starƟng from any seed word you\n",
    "provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c79471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence is transforming\n",
      "machine advances in machine\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "text = \"\"\"Artificial intelligence is transforming industries by automating tasks and enabling smarter decision-making. \n",
    "From self-driving cars to personalized recommendations on streaming platforms, AI is deeply embedded in our daily lives. \n",
    "It helps in improving efficiency, reducing human error, and offering insights that were previously unattainable. \n",
    "With advances in machine learning, deep learning, and natural language processing, the capabilities of AI continue to expand. \n",
    "As technology evolves, responsible and ethical AI development is critical to ensuring benefits are distributed fairly and do not cause harm.\"\"\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in text.split('.'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "\n",
    "xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "ys = np.zeros((len(labels), total_words))\n",
    "for i, label in enumerate(labels):\n",
    "    ys[i][label] = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(xs, ys, epochs=500, verbose=0)\n",
    "\n",
    "def generate_text(seed_text, next_words=3):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted = np.argmax(predicted_probs, axis=-1)[0]\n",
    "        \n",
    "        output_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += ' ' + output_word\n",
    "    return seed_text\n",
    "\n",
    "print(generate_text(\"artificial\"))\n",
    "print(generate_text(\"machine\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
